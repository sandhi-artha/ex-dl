# ML Pytorch Baseline
(I think) I wanted to create a working baseline for most common DL tasks using CIFAR10, a quick easy dataset I'm aware of. Purpose is:
- practice pipelines for different models, task
- have working model to explore other configs

# Design

## Components
### Model
- base: `loss_function` and `forward` function
- classification
- autoencoders

### Module

## tasks
- CNN
- AutoEncoders
- Deep Clustering

## Models
- CNN
- ViT
- AE
- custom

# Timings
```
epoch   lr      t_train loss    acc     t_val   val_los val_acc t_total
   0    0.0360  9.2533  1.9422  0.2745  2.5297  0.0046  0.0009     11.78
   1    0.0681  7.8026  2.0018  0.2384  2.4183  0.0056  0.0003     22.00
   2    0.1000  7.8040  2.2334  0.1389  2.4940  0.0059  0.0003     32.30
   3    0.0941  7.8331  2.3073  0.1000  2.4929  0.0059  0.0003     42.63
   4    0.0882  7.9921  2.3077  0.1000  2.4507  0.0059  0.0003     53.07
   5    0.0823  7.7439  2.3083  0.1000  2.5427  0.0059  0.0003     63.36
   6    0.0765  8.0029  2.3081  0.1000  2.5731  0.0059  0.0003     73.93
   7    0.0706  7.8973  2.3068  0.1000  2.4141  0.0059  0.0003     84.24
   8    0.0647  7.7907  2.3062  0.1000  2.4718  0.0059  0.0003     94.51
   9    0.0588  7.9793  2.3068  0.1000  2.6104  0.0059  0.0003    105.10
  10    0.0529  7.9632  2.3062  0.1000  2.6168  0.0059  0.0003    115.68
  11    0.0470  8.0311  2.3057  0.1000  2.4371  0.0059  0.0003    126.15
  12    0.0412  7.8783  2.3055  0.1000  2.4764  0.0059  0.0003    136.50
  13    0.0353  7.8744  2.3052  0.1000  2.4914  0.0059  0.0003    146.87
  14    0.0294  7.7957  2.3050  0.1000  2.5771  0.0059  0.0003    157.24
  15    0.0235  7.8675  2.3044  0.1000  2.5968  0.0059  0.0003    167.70
  16    0.0176  7.8683  2.3039  0.1000  2.3952  0.0059  0.0003    177.97
  17    0.0118  7.6455  2.3034  0.1000  2.4108  0.0059  0.0003    188.02
  18    0.0059  7.6473  2.3032  0.1000  2.3852  0.0059  0.0003    198.05
  19    -0.0000 7.6559  2.3029  0.1000  2.3984  0.0059  0.0003    208.11
```
using cache_ds, timings improve significantly
```
epoch   lr      t_train loss    acc     t_val   val_los val_acc t_total
   0    0.0360  5.5762  1.9177  0.2830  2.3387  0.0043  0.0008      7.91
   1    0.0681  4.3350  1.9915  0.2542  2.3345  0.0053  0.0005     14.58
   2    0.1000  4.2390  2.2446  0.1266  2.3665  0.0059  0.0003     21.19
   3    0.0941  4.3253  2.3084  0.1000  2.3360  0.0059  0.0003     27.85
   4    0.0882  4.6040  2.3088  0.1000  2.4182  0.0059  0.0003     34.87
   5    0.0823  4.5376  2.3081  0.1000  2.4860  0.0059  0.0003     41.90
   6    0.0765  4.3021  2.3073  0.1000  2.3590  0.0059  0.0003     48.56
   7    0.0706  4.3364  2.3073  0.1000  2.4369  0.0059  0.0003     55.33
   8    0.0647  4.3243  2.3069  0.1000  2.3790  0.0059  0.0003     62.03
   9    0.0588  4.2354  2.3068  0.1000  2.3966  0.0059  0.0003     68.67
  10    0.0529  4.5795  2.3057  0.1000  2.4386  0.0059  0.0003     75.68
  11    0.0470  4.3962  2.3061  0.1000  2.4129  0.0059  0.0003     82.49
  12    0.0412  4.4858  2.3053  0.1000  2.3716  0.0059  0.0003     89.35
  13    0.0353  4.3269  2.3051  0.1000  2.4523  0.0059  0.0003     96.13
  14    0.0294  4.2733  2.3046  0.1000  2.3277  0.0059  0.0003    102.73
  15    0.0235  4.2586  2.3047  0.1000  2.3459  0.0059  0.0003    109.34
  16    0.0176  4.2484  2.3039  0.1000  2.3507  0.0059  0.0003    115.93
  17    0.0118  4.2716  2.3038  0.1000  2.3212  0.0059  0.0003    122.53
  18    0.0059  4.2062  2.3031  0.1000  2.3373  0.0059  0.0003    129.07
  19    -0.0000 4.4676  2.3030  0.1000  2.3352  0.0059  0.0003    135.87
```

# Env
`torch-lg` with pytorch 2.0.0

# What to cover
- Normalization: what if you leave:
    - as FP32 with random scaling
    - as uint8 without normalizing
    - as uint8 with standardization (rescale to 0-1)
    - calculate mean and std of the dataset and use it for normalization
- Optimization:
    - how to read gpu utilization, memory usage
    - how to optimize dataset (cache and not)
- Metrics:
    - some Torch metrics are Classes, some are functions